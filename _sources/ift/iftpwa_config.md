# iftpwa - Default Configuration

## Correlated Field Model in NIFTy
Please read the following first, perhaps run the code yourself to see how the correlated field model works in NIFTy. This model forms the basis of the iftpwa package.
[Showcasing the Correlated Field Model (NIFTy)](https://ift.pages.mpcdf.de/nifty/user/old_nifty_getting_started_4_CorrelatedFields.html)

```{note}
The correlated field model is no longer a Gaussian process but it does make understanding and explaining the IFT approach easier
```

## Simplified Model

The following simplified amplitude description is used to depict how the configuration file key-value fields affect the model for each amplitude. The first term in the brackets describes a non-parameteric component which can be adept at describing unknown background contributions. The second term describes a parameteric component which we generally have a physical description for (i.e. Breit-Wigner, FlattÃ©, ...)

$$
\begin{aligned}
A_i(m, t) = \kappa \cdot \rho_i \cdot C_i \cdot \Big[ 
    & S_b \cdot G(m \mid s_{m,i},\; f_{m,i},\; a_{m,i}) \cdot G(t \mid s_{t,i},\; f_{t,i},\; a_{t,i}) \cdot I_i \\
    & + S_r \sum_p \cdot\; S_{p,i} \cdot P_{p,i}(m \mid \vec{x_{p,i}}) \cdot T_{p,i}(t)
\Big]
\end{aligned}
$$

- $m$ is the mass
- $t$ is the transfer momentum
- $i$ is the wave index
- $\kappa$ is a constant kinematic factor. Expression hardcoded in `iftpwa/src/model/model_builder.py`, under development for GlueX. Currently just squared barrier factor.
- $S_b$ is the constant `bkg2res` factor scaling ALL background contributions (all waves) by the same amount
- $S_r$ is the constant `res2bkg` factor scaling ALL parametric contributions (all waves) by the same amount
- $\rho_i$ is the dictionary of constant phase space factors for the $i^{th}$ partial wave stored in a single pkl file, `phaseSpaceMultiplier`. Can be generated by `pa calc_ps` command
- $C_i$ is the overall scale factor for the $i^{th}$ partial wave. This is a random variable with half-normal / laplace priors. Without this, every amplitude should be O(1) scale, defined by `IFT_MODEL.scale`
- $G(s_i, f_i, a_i)$ is the Correlated field model with scale, flexibility, asperity, for the $i^{th}$ partial wave defined by `IFT_MODEL`. These parameters are random variables and are log-normal distributed. Mass and transfer momentum is factorized as a product of two correlated fields
- $I_i$ is the indicator function to zero the Correlated field component for a particular partial wave, indicated by `no_bkg` key in each resonance model. Useful if only want parameteric component in a specific wave
- $\sum_p$ sums over parametric components (multiple components can contribute to a single partial wave) defined by `PARAMETRIC_MODEL`
- $S_p$ is the constant `preScale` factor for the particular parametric component, $p$. Correlated field model at this point is roughly O(1) so this allows us to bias towards larger/smaller intensity from parameteric component
- $P(m \mid \vec{x_{p,i}})$ is the `PARAMETRIC_MODEL` component describing the mass dependence of the parameteric component with parameters $\vec{x_{p,i}}$ defined by `parameter_priors` (and also `static_paras`)
- $T(t)$ models the t-dependence for the Parameteric term. It can be another correlated field (with hardcoded power spectrum parameters) if `PARAMETRIC_MODEL.smoothScales` is true

```{note}
This amplitude description is for a single real/imaginary component of a particular partial wave. The correlated field parameters (defined as priors), $s_i, f_i, a_i$, are shared across all partial waves. This does not mean all the particular $s_i, f_i, a_i$ across partial waves are the same, as they are random variables. Though the real/imaginary components of a partial wave do share the same parameter values. 
```

## Default Configuration

We almost always would like to compare the IFT results to a set of maximum likelihood fits obtained using `AmpTools` so we need to perform two types of fits. A set of Maximum Likelihood (MLE) fits is performed using `AmpTools` and an Information Field Theory (IFT) fit is performed using `iftpwa`. Each has its own configuration file to specify the initial state. `PyAmpTools` will be the main driver of both these fits and has a YAML key that points to the `iftpwa` configuration file. Likewise, the `iftpwa` configuration file has a field that points to the `PyAmpTools` configuration file as it uses it to construct a `pwa_manager` object that calculates likelihoods.

There are two configuration files that one needs to manage.  We call them `pyamptools.yaml` and `iftpwa.yaml` but they can be anything. 
- `PyAmpTools` uses a configuration file to define the reaction, dataset locations, partial waves used, and kinematic binning
- `iftpwa` uses a configuration file but in this case to declare its IFT model, how to modify its likelihood (bootstrap, resampling, ...), and how to perform the IFT optimization.

Below are the default YAML configurations for GlueX analyses of $\gamma p \rightarrow \eta\pi^0 p$. This channel forms a basis for all **two pseudoscalar** channels as the partial wave basis is the same so the much of the general configuration is shared. 

```{note}
**Vector pseudoscalar** channels are also supported. The IFT model description remains the same and the associated default settings can work well. The only thing we have to consider is how to tell `PyAmpTools` how to generate the required `AmpTools` configuration file. See bottom of page for more information, which can also help other channels use `PyAmpTools` to generate a usable `AmpTools` configuration file. 
```

### PyAmpTools Configuration

`PyAmpTools` enforces a specific format for how to name amplitudes given some quantum numbers (stored as a dictionary in `pyamptools/src/utilities/general.py`). It also expects several files in the directory specfied in the `data_folder` YAML key with specific naming conventions also. This restriction allows `PyAmpTools` to bin the data sources and generate a configuration file for each bin that `AmpTools` can use. `iftpwa` can handle much finer kinematic bins than AmpTools due to the prior's regularization effect. To account for this, we split the data into fine bins, usable by `iftpwa`, then regroup the data bins into coarser bins for `AmpTools`.

```yaml
# Example data folder structure
accmc.root # shares this acceptance MC dataset for all polarizations
data000.root # data MC datasets for each polarization
data045.root # data MC datasets for each polarization
data090.root # data MC datasets for each polarization
data135.root # data MC datasets for each polarization
genmc.root # shares this generated/thrown MC dataset for all polarizations
bkgnd000.root # (optional) background MC datasets for each polarization
bkgnd045.root # (optional) background MC datasets for each polarization
bkgnd090.root # (optional) background MC datasets for each polarization
bkgnd135.root # (optional) background MC datasets for each polarization
```

```{note}
NIFTy results are regularized so it is able to handle finer kinematic binning than binned MLE fits. To bridge the two, we can initially bin finely then regroup nearby bins for AmpTools by specifying the `amptools.bins_per_group` key. This key must be a divisor of `n_mass_bins`. For the default settings below, we double the bins NIFTy sees over what AmpTools sees.
```

```yaml
# EXAMPLE: pyamptools.yaml
defaults_location: null # [Ignore] feature not yet implemented
base_directory: /MY/WORKING/DIRECTORY # Dump ALL results relative to this directory
data_folder: /MY/DATA/FOLDER # Path to the data, accmc, bkgnd, genmc datasets
polarizations:
    '000': 0.3519 # (float) polarization fraction in each polarization: (000, 045, 090, 135) 
    '045': 0.3374
    '090': 0.3303
    '135': 0.3375
waveset: Dm1-_Dm2-_Dp0-_Dp1-_Dp2-_Sp0-_Dm1+_Dm2+_Dp0+_Dp1+_Dp2+_Sp0+_Pm1+_Pp0+_Pp1+_Pm1-_Pp0-_Pp1- # (string) all the waves to consider
phase_reference: Dp2+_Dm1- # (string) use these phase references when making plots. A single wave must be provided for each incoherent sector (e.g. +/- reflectivity state) which will be used as the reference for all plotted waves in that sector
reaction: Beam Proton Pi0 Eta # AmpTools reaction scheme
daughters: # Define daughter particles and masses. NOTE: phase space calculation will crash ndaughters !=2. In this case, set phaseSpaceMultiplier to null and do not call `pa calc_ps` later on
    Pi0: 0.135 # (float) mass of the daughter particle 1
    Eta: 0.548 # (float) mass of the daughter particle 2
acceptance_correct: true # (bool) plot acceptance corrected data and fit results
min_mass: 0.9 # (float) minimum mass
max_mass: 2.1 # (float) maximum mass
n_mass_bins: 80 # (int) number of mass bins
min_t: 0.1 # (float) minimum transfer momentum 
max_t: 1.0 # (float) maximum transfer momentum
n_t_bins: 1 # (int) number of t bins
init_one_val: null # (float) all amplitudes are initialized to this value for AmpTools configuration file
datareader: ROOTDataReader # (string) AmpTools data reader to use
coordinate_system: cartesian # (string) coordinate system to use: {cartesian, polar}. 'polar' likely will not work.
real_waves: '' # (string) same form as waveset, define which waves are purely real
fixed_waves: '' # (string) same form as waveset, define which waves are fixed
add_amp_factor: '' # (string) Add an amplitude factor to every amplitude. For example, OmegaDalitz 0.1212 0.0257 0.0 0.0 will create a factor for the dalitz decay of the omega
append_to_decay: '' # (string) append this string to the decay amplitude, i.e. 'omega3pi' can be appended to Vec_ps_refl amplitude definition
append_to_cfg: '' # (string) dump contents to the bottom of the AmpTools configuration file
amptools: # Configure AmpTools
    output_directory: ${base_directory}/AmpToolsFits # (string) directory to dump AmpTools results into
    search_format: group # (string) calculate fit fractions for the AmpTools results for the grouped data bins ('group') or every data bins ('bin')
    n_randomizations: 10 # (int) number of random initialization fits to perform
    mle_query_1: status == 0 & ematrix == 3 # (string) query 1 to filter only minuit converged and positive definite error matrices
    mle_query_2: delta_nll==0 # (string) delta_nll is calculated after query 1, apply this query to select the best fit
    bins_per_group: 2 # (int) regrouping number, iftpwa uses finer bins than AmpTools. Must be divisible by n_mass_bins!
    merge_grouped_trees: true # (bool) remerge the trees in each group
    constrain_grouped_production: false # (bool) if not remerging, we have the choice of constraining the amplitudes in each bin of a group to be the same
    regex_merge: --regex_merge '.*reaction_(000|045|090|135)::(Pos|Neg)(?:Im|Re)::' # (string) `pa fitfrac` argument to calculate fit fractions for certain coherent sums following this replacement pattern. See demos/extract_ff.ipynb for more information on the usage
    prepare_for_nifty: true # (bool) iftpwa require fixed scales between datasets (i.e. polarized datasets). This will modify AmpTools config to handle this
    n_processes: 20 # (int) pool of processes to analyze all the amptools fits. MLE fits are quite cheap so this does not have to be large
nifty: # Configure NIFTy
    output_directory: ${base_directory}/NiftyFits # (string) directory to dump NIFTy results into
    yaml: /LOCATION/OF/IFTPWA/YAML/FILE/iftpwa.yaml # (string) path to the iftpwa configuration file
    synchronize: true # (bool) Almost always true. Synchronize the iftpwa and PyAmpTools configuration files
    mpi_processes: 41 # (int) number of MPI processes to use for NIFTy. Choice depends on number of cores available and n_mass_bins * n_t_bins

############ [OPTIONAL] ################
# dash: # Configure dash plots
#     t: 0.55 # (float) t-bin to draw the dash plots for
#     models:
#         my_first_fit: /PATH/TO/COMPLETED/NIFTY/FIT/CONTAINING/NIFTYPWA_FIT.PKL # (string) path to the completed NIFTy fit
#     cache_loc: .dash_ift_cache.pkl # (string) cache the dash plots for faster loading
#     no_browser: true # (bool) if true, the dash plots will not open in the browser
#     pdf_dump: dash_iftpwa.pdf # (string) path to the pdf dump
#     html_dump: dash_iftpwa.html # (string) path to the html dump
#     html_dump_static: false # (bool) if true, the html dump will be static, otherwise it will be interactive
#     acceptance_correct: true # (bool) if true, the dash plots will be acceptance corrected amplitude values
#     ntasks: 10 # (int) number of tasks to run in parallel for faster processing
```

### IFTPWA Configuration

```{note}
NIFTy provides [Metric Gaussian Variational Inference](https://arxiv.org/abs/1901.11033) and [Geometric Variational Inference](https://arxiv.org/abs/2105.10470) methods for optimization. Both of these approaches alternate between optimizing the KL divergence for a specific shape of the variational posterior and updating the shape of the variational posterior, see [here](https://github.com/NIFTy-PPL/NIFTy?tab=readme-ov-file). A *global iteration* (see `nIterGlobal` key below) is complete when both optimization steps conclude. Some reshaping of the posterior is performed to make it more Gaussian-like for more effective inference.
```

```yaml
# EXAMPLE: iftpwa.yaml
GENERAL:
    pwa_manager: GLUEX
    seed: 42 # (int) RNG seed for reproducible results. Optimizer can get stuck in local minima so freely change this or try multiple values
    verbosityLevel: 1 # (int) Greater than 0 will print a bit (slightly) more information to screen
    outputFolder: ??? # (path) Base folder to dump all results into
    fitResultPath: ${GENERAL.outputFolder}/niftypwa_fit.pkl # (path) Python pickle file dump location, pkl file stores all the information about {input, model, result}
    maxNIFTyJobs: 1 # (int) Number of NIFTy jobs (not to be confused with MPI processes that the PWA_MANAGER can spawn). For example, if one would like to use multiple NIFTy processes for sampling. Untested.
    initial_position_std: 0.1 # (float) Random initial position is drawn from your prior model, Gaussian distributed with this standard deviation. See NIFTy/src/minimization/optimize_kl.py for more information
    # initial_position_template: /w/halld-scshelf2101/lng/WORK/PyAmpTools9/OTHER_CHANNELS/ETAPI0/RESULTS_JAX_P/NiftyFits_a2B_a2pB_a0F/niftypwa_fit.pkl
    default_yaml: null # (path) values in this file can be used to update a yaml file found at the path specified with this YAML key. Allows reusage.

# pwa_manager for GlueX takes in another configuration file that defines the partial waves used and the kinematic binning: (mass, t)
#   This maintains consistency between iftpwa results and results from a set of maximum likelihood fits obtained using AmpTools
PWA_MANAGER:
    yaml_file: /PATH/TO/PYAMPTools/YAML/FILE/pyamptools.yaml

##########################################
# NOTE: DEFAULTS ARE DECENT STARTING POINT
##########################################
# Define our forward model containing correlated fields and parametric models
IFT_MODEL:
    scale: auto # (float, "auto") overal scale factor for amplitude
    nCalcScale: 500 # (float) number of samples used to estimate IFT_MODEL.scale
    positiveScale: half_normal # (half_normal, sqrt_exp) or (bool) choice of distribution using IFT_MODEL.scale. half_normal is a half normal, sqrt_exp is laplace (sqrt since we are on amplitude level not intensity). Bool is legacy to take absolute value of scale
    useLogNormalPriorSlope: false # (bool) use log normal prior for the slope of the power spectra instead of normal
    useGammaFlex: false # (bool) use Gamma distribution for the flexibility of the power spectra. Typically lighter tails than log-normal
    useLaplaceScale: false # (bool) should be same as setting positiveScale = sqrt_exp. We generally only consider positive scales so this is technically exponentially distributed
    useGammaAsp: false # (booL) use Gamma distribution for the asperity of the power spectra. Typically lighter tails than log-normal
    loglogavgslope: [-4.0, 0.1] # [mass axis] (float, float) for (mean, std) of the power spectrum slope (log-log scale). More negative slopes will limit high frequency components. Must be provided.
    flexibility: [0.5, 0.5] # [mass axis] - (float, float) for (mean, std) or null. (0.5, 0.5) is special case resulting in Laplace distribution. Determines the amplitude of the integrated Wiener process component of the power spectrum.
    asperity: [0.5, 0.5] # [mass axis] - (float, float) for (mean, std) or null. (0.5, 0.5) is special case resulting in Laplace distribution. Determines the roughness of the integrated Wiener process component of the power spectrum.
    loglogavgslopeTprime: [-4.0, 0.1] # [t axis] (float, float). Cannot be null if you have multiple t-bins. 
    flexibilityTprime: [0.5, 0.5] # [t axis] - (float, float) or null. Read above
    asperityTprime: null # [t axis] (float, float) or null. Read above
    stdOffset: [1.0, 0.001] # Amplitude offfset is normally distributed around 0 with this (float, float) for (mean, std). 
    dofdexStye: real_imag # (real_imag, single_dofdex) allows fields to share the same power spectrum model. Useful for coupling real and imaginary parts of the same complex amplitude
    custom_model_path: ${PARAMETRIC_MODEL} # (path, YAML reference, null). Path must point to a file (see iftpwa/assets/parametric_model.py). (YAML reference) defined below. Set null if you do not want to use any parametric models
    modelName: null # (string) this key is needed to select a model from the model dictionary if custom_model_path is a file path. null if custom_model_path is defined in this YAML file as in this example
    res2bkg: 1.0 # (float) globally scale ALL parametric/resonance contributions before adding to bkgnd to form the signal
    bkg2res: 1.0 # (float) globally scale ALL bkgnd contributions before adding to parametric/resonance to form the signal
    tPrimeFalloff: 0 # (float) preweight t distribution. Legacy
    estimateRatios: false # (bool) use the user-provided dataset ratios. Unused by GlueX
    perTprimeScaling: true # (bool) scale each t bin separately
    altTprimeNorm: no_norm # (peak_norm, no_norm, kin_norm) dictates how to normalize the phase space multiplier for each t-bin. no_norm keeps the normalization the supplied phase space pkl file
    noInterpScipy: false # (bool) use scipy.interpolate.interp1d cubic interpolation instead of nifty.interp linear interpolation to ensure values align in the phase space multiplier
    phaseSpaceMultiplier: null # (path) or null. Path to a pickled file containing a tuple of (masses, dict[amp: float]) where each float will scale the corresponding amp field. Generated by `pa calc_ps`
    s0: null # (float) or null. Energy scale factor. 
    productionFactor: GLUEX # (string) or null. Production factor scheme (see iftpwa/src/mode/model_builder.py). Depends on beam energy and target mass. 
    ####### BELOW ARE ONLY USED IF USING MULTIPLE DATASETS #######
    ratio_fluct: null # can use if using multiple datasets
    ratio_fluc_std: null # can use if using multiple datasets
    ratio_slope: null # can use if using multiple datasets
    ratio_stdSlope: null # can use if using multiple datasets
    ratio_slopeTprime: null # can use if using multiple datasets
    ratio_stdSlopeTprime: null # can use if using multiple datasets
    ratio_std_offset: null # can use if using multiple datasets 
    ratio_std_std_offset: null # can use if using multiple datasets

######################################################################
# NOTE: USED ONLY IF IFT_MODEL.custom_model_path = ${PARAMETRIC_MODEL}
######################################################################
PARAMETRIC_MODEL:
    smoothScales: False # (bool) uses a correlated field to smooth the t-dependence of the parameteric models
    parameter_priors: # Define prior distributions for the free parameters 
        # see iftpwa/src/utilities/nifty.py for available priors but most parameters should be positive so log-normal is typical
        m_a0_980: UnnamedLogNormalOperator(sigma=0.05, mean=0.98) # Flatte peak mass for a0(980)
        g1_a0_980: UnnamedLogNormalOperator(sigma=0.1, mean=0.35) # Flatte coupling_1 for a0(980)
        g2_a0_980: UnnamedLogNormalOperator(sigma=0.1, mean=0.35) # Flatte coupling_2 for a0(980)
        m_a2_1320: UnnamedLogNormalOperator(sigma=0.0013 * 30, mean=1.3186) # Breit-Wigner peak mass for a2(1320)
        w_a2_1320: UnnamedLogNormalOperator(sigma=0.002 * 30, mean=0.105) # Breit-Wigner width for a2(1320)
        m_a2_1700: UnnamedLogNormalOperator(sigma=0.02, mean=1.706) # Breit-Wigner peak mass for a2(1700)
        w_a2_1700: UnnamedLogNormalOperator(sigma=0.05, mean=0.380) # Breit-Wigner width for a2(1700)
    resonances:
    - a0_980:
        name: "$a_0(980)$" # (string) name of the resonance in LaTeX format
        fun: "flatte" # (string) name of the function to use for the resonance. Available functions found at iftpwa/src/model/physics_functions.py
        preScale: 3.5 # preScale this component by this factor to bias towards total values. Can be 0
        no_bkg: false # (bool) if true, this component will not have a correlated field background component
        paras: {"mass": m_a0_980, "g1": g1_a0_980, "g2": g2_a0_980} # (dict) free parameters for the physics function
        static_paras: {"mass11": 0.548, "mass12": 0.135, "mass21": 0.495, "mass22": 0.495, channel: 1} # (dict) fixed parameters for the physics function
        waves: ['Sp0-', 'Sp0+'] # (List[string]) list of waves containing this parameteric component
    - a2_1320:
        name: "$a_2(1320)$"
        fun: "breitwigner_dyn"
        preScale: 1.5
        no_bkg: False
        paras: {"mass": m_a2_1320, "width": w_a2_1320}
        static_paras: {"spin": 2, "mass1": 0.548, "mass2": 0.135}
        waves: ['Dm2-', 'Dm1-', 'Dp0-', 'Dp1-', 'Dp2-', 'Dm2+', 'Dm1+', 'Dp0+', 'Dp1+', 'Dp2+']
    - a2_1700:
        name: "$a_2(1700)$"
        fun: "breitwigner_dyn"
        preScale: 5
        no_bkg: false
        paras: {"mass": m_a2_1700, "width": w_a2_1700}
        static_paras: {"spin": 2, "mass1": 0.548, "mass2": 0.135}
        waves: ['Dm2-', 'Dm1-', 'Dp0-', 'Dp1-', 'Dp2-', 'Dm2+', 'Dm1+', 'Dp0+', 'Dp1+', 'Dp2+']

#####################################################################################################################################
# NOTE: {nSamples, nIterGlobal, nMultiStart, multiStartObjective} ARE THE MOST IMPORTANT PARAMETERS
#       THE OPTIMIZER SETTINGS FOR (KL MINIMIZATION, SAMPLING, GEOVI) HAVE DECENT DEFAULTS
#####################################################################################################################################
# Some parameters takes a list of lists which dictate the number of global iterations in a given setting
#   See iftpwa/src/scripts/iftpwa_fit.py for calls to `makeCallableSimple` for parameters with this style
OPTIMIZATION:
    nSamples: [[1, 0], [4, 5], 25] # (int) Runs 1 global iteration with 0 samples (point estimate), then 4 global iterations with 5 samples each, then 25 samples for all remaining global iterations
    nIterGlobal: 1 # (int) number of global iterations to run NIFTy optimize_kl for
    nMultiStart: 0 # (int) number of multiple restarts to perform (each only a fixed single global iteration) that optimizes multiStartObjective. This pair of keys allows a cheap search over initial conditions
    multiStartObjective: "minimize|energy" # (str) pipe separated direction|quantity to select best starting condition. i.e. "maximize|Dp2+_fit_intens" maximizes the intensity of the Dp2+ amplitude
    
    # KL Minimizer
    algoOptKL: LBFGS # (string) algorithm
    nIterMaxOptKL: 50 # (int) number of iterations to perform algoOptKL to optimize the KL divergence
    deltaEKL: 0.001 # (float) if the difference between the last and current energies is below this value, the convergence counter will be increased in this iteration.
    convergenceLvlKL: 2 # (int) the number which the convergence counter must reach before the iteration is considered to be converged

    # Sampling
    # The accuracy of the samples specify the point at which they become dominated by noise. As to not constrain the KL minimization
    #    samples should be sampled more accurately than our KL optimization objective, hence, the more stringent minimization criterion for the sampling in the demonstration script
    #    See https://gitlab.mpcdf.mpg.de/ift/nifty/-/blob/NIFTy_8/demos/0_intro.py and https://github.com/NIFTy-PPL/NIFTy/issues/35
    nIterMaxSamples: 1000 # (int) iteration limit for sampling
    deltaESampling: 0.001 # (float) see above
    convergenceLvlSampling: 2 # (int) see above

    # GeoVI
    algoOptGeoVI: Newton # (string) algorithm
    nIterMaxGeoVI: 10 # (int) number of iterations to perform algoOptGeoVI
    deltaEGeoVI: 0.001 # (float) see above
    convergenceLvlGeoVI: 2 # (int) see above

    #############
    resume: true # (bool) resume previous fit or overwrite
    #############

    niftyCache: ${GENERAL.outputFolder}/ # (path) path to dump NIFTy results
    overwriteCache: true # (bool) overwrite the cache
    sanityChecks: false # (bool) NIFTy - some sanity checks that are evaluated at the beginning. They are potentially expensive because all likelihoods have to be instantiated multiple times
    constants: [tamps_mass_dir_loglogavgslope] # provide the names of the latent space variables that should be fixed, examples: ['tamps_mass_dir_loglogavgslope', 'tamps_mass_dir_flexibility', 'tamps_tprime_dir_loglogavgslope', 'tamps_tprime_dir_flexibility', 'tamps_zeromode']

##########################################
# NOTE: DEFAULTS ARE DECENT STARTING POINT
##########################################
# Some parameters takes a list of lists which dictate the number of global iterations in a given setting
#   See iftpwa/src/scripts/iftpwa_fit.py for calls to `makeCallableSimple` for parameters with this style
LIKELIHOOD:
    approximation: false # (bool) approximate the likelihood (with Gaussian, I believe)
    metric_type: normal # (normal, studentt) type of metric to use for the likelihood
    theta_studentt: null # (float) theta paraemeter for student-t distribution, this has to be set if metric_type is studentt
    learn_metric: false # (bool) can learn the metric but not recommended
    learn_metric_sigma: 1.0 # (float)
    nKeepMetric: 1 # (int) global iterations at nKeepMetric intervals will not approximate the likelihood. 1 means we never approximate the likelihood I guess
    subSampleProb: 1.0 # (float) subsample the data to speed up likelihood calculation
    initialSubSampleProb: 1.0 # (float) scales the initial signal field by this factor, in principle should match the initial subSampleProb
    subSampleApproxIndep: false # (bool) Legacy, unused
    bootstrap: false # (bool) bootstrap the data to introduce more stochasticity
    external_approximation: null # (path) pointing to an external pickle file, null if not used
    dropout_prob: 0.0 # (float) dropout probability for each kinematic bin. Introduces more stochasticity and can help with overfitting
    rotate_phase: null # (bool) or null. Unused
    clip_latent: 0 # (float) if between [0, 1] then this is a fractional scaling of the latent space between (-clip_latent, clip_latent) to prevent large updates. 

#######################################################
# NOTE: RUN SCANS BY PASSING --hyperopt to `pa run_ift`
#######################################################
# Scans are performed by [Optuna](https://github.com/optuna/optuna). 
# - iftpwa naturally tracks the likelihood energy and the intensity of all individual amplitude components. 
# - if a ground truth is provided, the mean squared error or chi-squared (if number of samples > 1) are also calculated. 
# Above metrics are all available to be optimized against. These metrics take the form `{amplitude_field_name}_{"fit", resonance_name, "bkg"}_{"intens", "chi2"}`. 
# - A list of tracked metrics can be dumped at the end of a iftpwa fit if `verbosityLevel` > 0 if you are unsure
# Aggregated stats for `chi2` metric is also calculated: `{min, -2sigma, -1sigma, median, 1sigma, 2sigma, max}` if available
# Fields within this yaml can be referenced following a field path that is period delimited. 
# An Optuna `suggest` command and associated args must also be passed. Field values must be a string that is then `eval`'d by `python` to construct `args` and `kwargs`. 
# - For example, `OPTIMIZATION.nSamples.1|suggest_int: "2, 10, step=2"` would modify the `nSamples` sub-field within the `OPTIMIZATION` field so that Optuna makes integer suggestions on list index element 1. The suggestions are integers between 2 and 10 in steps of 2. 
# - See Optuna docs for available `suggest` commands.
HYPERPARAMETER_SCANS:
    n_trials: 20
    objective: "minimize|energy" # (str) direction|objective where direction is either {'minimize', 'maximize'}
    sampler: RandomSampler # (str) see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. BruteForceSampler can be used to exhaust all combinations
    GENERAL.seed|suggest_int: "0, 10000, step=1" # (str) randomizing the seed can be a proxy for running randomly initialized fits
    IFT_MODEL.loglogavgslope.1|suggest_float: "0.01, 0.51, step=0.25"
    IFT_MODEL.res2bkg|suggest_float: "0.01, 1.01, step=0.25"
    IFT_MODEL.scale|suggest_int: "1000, 21000, step=4000"
```

## Vector Pseudoscalar Channels

The IFT model description remains the same and the default settings can work well. The only thing we have to consider is how to tell `PyAmpTools` how to generate the required `AmpTools` configuration file and of course use a partial wave naming scheme that it can understand. Below is an example of for the $\gamma p \rightarrow \omega\eta p \rightarrow \eta \pi^+ \pi^- \pi^0 p$ channel. 

Here we only highlight *some* specific keys that are different from the default

```yaml
waveset: 1Sp1-_1Sp1+_1Sp0-_1Sp0+_1Pp1-_1Pp1+_1Pp0-_1Pp0+_2Pp0+_2Pp0-_2Pp1+_2Pp1-_2Pp2+_2Pp2-_3Fp0+_3Fp0-_3Fp1+_3Fp1-_3Fp2+_3Fp2-_3Fp3+_3Fp3-
phase_reference: 1Pp1-_1Pp1+
reaction: Beam Proton Eta Pi0 Pi+ Pi-
datareader: ROOTDataReaderTEM .1 .6 8.2 8.8 1.325 2.0
add_amp_factor: OmegaDalitz 0.1212 0.0257 0.0 0.0
append_to_decay: omega3pi
```